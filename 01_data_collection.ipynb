{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 â€” Data Collection (Binance OHLCV)\n",
        "\n",
        "This notebook fetches OHLCV for top crypto pairs from Binance and stores each field as separate Parquet files under `storage/ohlcv`. All file paths are relative for macOS/Linux."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running locally, ensure dependencies are installed:\n",
        "# pip install requests pandas pyarrow fastparquet tqdm\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime, timezone\n",
        "from typing import List, Dict\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Relative storage folder (macOS/Linux friendly)\n",
        "BASE_DIR = os.path.abspath(os.path.join(os.getcwd()))\n",
        "STORAGE_DIR = os.path.join(BASE_DIR, 'storage', 'ohlcv')\n",
        "os.makedirs(STORAGE_DIR, exist_ok=True)\n",
        "print(f'Storage directory: {os.path.relpath(STORAGE_DIR)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BINANCE_API = 'https://api.binance.com'\n",
        "SESSION = requests.Session()\n",
        "SESSION.headers.update({'User-Agent': 'crypto-alpha-lab/1.0'})\n",
        "\n",
        "def get_exchange_info() -> Dict:\n",
        "    url = f'{BINANCE_API}/api/v3/exchangeInfo'\n",
        "    r = SESSION.get(url, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def top_spot_symbols(quote_priority: List[str] = None, limit: int = 25) -> List[str]:\n",
        "    \"\"\"Return top liquid spot symbols by quote asset priority and filters.\n",
        "    We approximate \"top\" by focusing on common quote assets and active trading status.\n",
        "    \"\"\"\n",
        "    if quote_priority is None:\n",
        "        quote_priority = ['USDT', 'USDC', 'FDUSD', 'BTC', 'ETH']\n",
        "    info = get_exchange_info()\n",
        "    symbols = [s for s in info.get('symbols', []) if s.get('status') == 'TRADING' and s.get('isSpotTradingAllowed')]\n",
        "    # Rank symbols by quote asset priority and base asset alphabetically as a tie-breaker\n",
        "    def score(sym):\n",
        "        q = sym.get('quoteAsset')\n",
        "        return (quote_priority.index(q) if q in quote_priority else 999, sym.get('baseAsset', ''))\n",
        "    ranked = sorted(symbols, key=score)\n",
        "    picked = []\n",
        "    seen_bases = set()\n",
        "    for s in ranked:\n",
        "        sym = s['symbol']\n",
        "        # Skip leveraged/index/fiat-like instruments by simple heuristics\n",
        "        if any(x in sym for x in ['UP', 'DOWN', 'BEAR', 'BULL']):\n",
        "            continue\n",
        "        if s.get('quoteAsset') not in quote_priority:\n",
        "            continue\n",
        "        # Prefer one quote per base to diversify the universe\n",
        "        base = s.get('baseAsset')\n",
        "        if base in seen_bases:\n",
        "            continue\n",
        "        seen_bases.add(base)\n",
        "        picked.append(sym)\n",
        "        if len(picked) >= limit:\n",
        "            break\n",
        "    return picked\n",
        "\n",
        "def klines(symbol: str, interval: str = '1d', limit: int = 1000, start_time: int = None, end_time: int = None) -> pd.DataFrame:\n",
        "    url = f'{BINANCE_API}/api/v3/klines'\n",
        "    params = {'symbol': symbol, 'interval': interval, 'limit': limit}\n",
        "    if start_time is not None: params['startTime'] = start_time\n",
        "    if end_time is not None: params['endTime'] = end_time\n",
        "    r = SESSION.get(url, params=params, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    cols = ['open_time','open','high','low','close','volume','close_time','quote_asset_volume','trades','taker_base_vol','taker_quote_vol','ignore']\n",
        "    df = pd.DataFrame(data, columns=cols)\n",
        "    if df.empty:\n",
        "        return df\n",
        "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms', utc=True)\n",
        "    df['close_time'] = pd.to_datetime(df['close_time'], unit='ms', utc=True)\n",
        "    num_cols = ['open','high','low','close','volume']\n",
        "    df[num_cols] = df[num_cols].astype(float)\n",
        "    return df[['open_time','open','high','low','close','volume','close_time']]\n",
        "\n",
        "def save_field_parquet(df: pd.DataFrame, symbol: str, field: str):\n",
        "    assert field in ['open','high','low','close','volume']\n",
        "    # Each field to its own parquet per symbol, under storage/ohlcv/{field}/{symbol}.parquet\n",
        "    field_dir = os.path.join(STORAGE_DIR, field)\n",
        "    os.makedirs(field_dir, exist_ok=True)\n",
        "    path = os.path.join(field_dir, f'{symbol}.parquet')\n",
        "    out = df[['open_time', field]].copy()\n",
        "    out = out.rename(columns={'open_time': 'timestamp', field: field})\n",
        "    out.to_parquet(path, index=False)\n",
        "    print(f'Saved {field} -> {os.path.relpath(path)} | rows={len(out)}')\n",
        "\n",
        "def save_all_fields(df: pd.DataFrame, symbol: str):\n",
        "    for f in ['open','high','low','close','volume']:\n",
        "        save_field_parquet(df, symbol, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch universe and OHLCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "symbols = top_spot_symbols(limit=30)\n",
        "print('Selected symbols:', symbols)\n",
        "\n",
        "all_counts = {}\n",
        "for sym in tqdm(symbols, desc='Downloading OHLCV (1d)'):\n",
        "    try:\n",
        "        df = klines(sym, interval='1d', limit=1000)\n",
        "        if df.empty:\n",
        "            print(f'No data for {sym}')\n",
        "            continue\n",
        "        save_all_fields(df, sym)\n",
        "        all_counts[sym] = len(df)\n",
        "        time.sleep(0.1)  # be gentle\n",
        "    except requests.HTTPError as e:\n",
        "        print(f'HTTP error for {sym}:', e)\n",
        "    except Exception as e:\n",
        "        print(f'Error for {sym}:', e)\n",
        "\n",
        "print('Completed symbols:', list(all_counts.keys()))\n",
        "print('Sample counts:', json.dumps({k: all_counts[k] for k in list(all_counts)[:5]}, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation: Reload parquet files and inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_field(symbol: str, field: str) -> pd.DataFrame:\n",
        "    path = os.path.join(STORAGE_DIR, field, f'{symbol}.parquet')\n",
        "    return pd.read_parquet(path)\n",
        "\n",
        "# Try up to first 3 symbols for quick validation\n",
        "check_syms = list(all_counts.keys())[:3] if 'all_counts' in globals() else []\n",
        "for sym in check_syms:\n",
        "    for f in ['open','high','low','close','volume']:\n",
        "        dfv = load_field(sym, f)\n",
        "        print(sym, f, dfv.shape, dfv.head(2).to_dict(orient='records'))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
