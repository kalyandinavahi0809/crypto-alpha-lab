{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "notebook-title",
      "metadata": {},
      "source": [
        "# 01 — Data Collection (Binance OHLCV)\n",
        "\n",
        "This notebook fetches OHLCV for top crypto pairs from Binance and stores each field as separate Parquet files under `storage/ohlcv`. All file paths are relative for macOS/Linux.\n",
        "\n",
        "**Features:**\n",
        "- Live data from Binance API (when internet is available)\n",
        "- Mock data mode for offline testing\n",
        "- Comprehensive error handling\n",
        "- Data validation and verification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-section",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup-imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running locally, ensure dependencies are installed:\n",
        "# pip install requests pandas pyarrow fastparquet tqdm\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from typing import List, Dict\n",
        "import random\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "USE_MOCK_DATA = False  # Set to True for offline testing with mock data\n",
        "\n",
        "# Relative storage folder (macOS/Linux friendly)\n",
        "BASE_DIR = os.path.abspath(os.path.join(os.getcwd()))\n",
        "STORAGE_DIR = os.path.join(BASE_DIR, 'storage', 'ohlcv')\n",
        "os.makedirs(STORAGE_DIR, exist_ok=True)\n",
        "print(f'Storage directory: {os.path.relpath(STORAGE_DIR)}')\n",
        "\n",
        "if USE_MOCK_DATA:\n",
        "    print('🧪 MOCK DATA MODE ENABLED - Using simulated data for testing')\n",
        "else:\n",
        "    print('🌐 LIVE DATA MODE - Will attempt to fetch real data from Binance API')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "helper-functions-section",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mock-data-classes",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mock data for testing without internet access\n",
        "MOCK_EXCHANGE_INFO = {\n",
        "    \"timezone\": \"UTC\",\n",
        "    \"serverTime\": 1645123456789,\n",
        "    \"symbols\": [\n",
        "        {\n",
        "            \"symbol\": \"BTCUSDT\",\n",
        "            \"status\": \"TRADING\",\n",
        "            \"baseAsset\": \"BTC\",\n",
        "            \"quoteAsset\": \"USDT\",\n",
        "            \"isSpotTradingAllowed\": True\n",
        "        },\n",
        "        {\n",
        "            \"symbol\": \"ETHUSDT\",\n",
        "            \"status\": \"TRADING\",\n",
        "            \"baseAsset\": \"ETH\",\n",
        "            \"quoteAsset\": \"USDT\",\n",
        "            \"isSpotTradingAllowed\": True\n",
        "        },\n",
        "        {\n",
        "            \"symbol\": \"ADAUSDT\",\n",
        "            \"status\": \"TRADING\",\n",
        "            \"baseAsset\": \"ADA\",\n",
        "            \"quoteAsset\": \"USDT\",\n",
        "            \"isSpotTradingAllowed\": True\n",
        "        },\n",
        "        {\n",
        "            \"symbol\": \"DOTUSDT\",\n",
        "            \"status\": \"TRADING\",\n",
        "            \"baseAsset\": \"DOT\",\n",
        "            \"quoteAsset\": \"USDT\",\n",
        "            \"isSpotTradingAllowed\": True\n",
        "        },\n",
        "        {\n",
        "            \"symbol\": \"SOLUSDT\",\n",
        "            \"status\": \"TRADING\",\n",
        "            \"baseAsset\": \"SOL\",\n",
        "            \"quoteAsset\": \"USDT\",\n",
        "            \"isSpotTradingAllowed\": True\n",
        "        },\n",
        "        # Add more symbols for testing\n",
        "        {\n",
        "            \"symbol\": \"BNBUSDT\",\n",
        "            \"status\": \"TRADING\",\n",
        "            \"baseAsset\": \"BNB\",\n",
        "            \"quoteAsset\": \"USDT\",\n",
        "            \"isSpotTradingAllowed\": True\n",
        "        },\n",
        "        {\n",
        "            \"symbol\": \"XRPUSDT\",\n",
        "            \"status\": \"TRADING\",\n",
        "            \"baseAsset\": \"XRP\",\n",
        "            \"quoteAsset\": \"USDT\",\n",
        "            \"isSpotTradingAllowed\": True\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "class MockSession:\n",
        "    \"\"\"Mock requests.Session for testing without internet access\"\"\"\n",
        "    def __init__(self):\n",
        "        self.headers = {}\n",
        "    \n",
        "    def get(self, url, params=None, timeout=None):\n",
        "        return MockResponse(url, params)\n",
        "\n",
        "class MockResponse:\n",
        "    \"\"\"Mock response object\"\"\"\n",
        "    def __init__(self, url, params=None):\n",
        "        self.url = url\n",
        "        self.params = params or {}\n",
        "        self.status_code = 200\n",
        "    \n",
        "    def raise_for_status(self):\n",
        "        pass\n",
        "    \n",
        "    def json(self):\n",
        "        if \"exchangeInfo\" in self.url:\n",
        "            return MOCK_EXCHANGE_INFO\n",
        "        elif \"klines\" in self.url:\n",
        "            return self._generate_mock_klines()\n",
        "        return {}\n",
        "    \n",
        "    def _generate_mock_klines(self):\n",
        "        \"\"\"Generate realistic mock OHLCV data\"\"\"\n",
        "        symbol = self.params.get('symbol', 'BTCUSDT')\n",
        "        limit = int(self.params.get('limit', 10))\n",
        "        \n",
        "        # Set base prices for different assets\n",
        "        price_map = {\n",
        "            'BTC': 50000, 'ETH': 3000, 'BNB': 400, 'ADA': 1.0, \n",
        "            'DOT': 7.0, 'SOL': 100, 'XRP': 0.6\n",
        "        }\n",
        "        \n",
        "        base_asset = next((k for k in price_map.keys() if k in symbol), 'BTC')\n",
        "        base_price = price_map[base_asset]\n",
        "        \n",
        "        data = []\n",
        "        end_time = datetime.now(timezone.utc)\n",
        "        \n",
        "        for i in range(limit):\n",
        "            timestamp = int((end_time - timedelta(days=limit-i-1)).timestamp() * 1000)\n",
        "            \n",
        "            # Generate realistic OHLCV with some randomness\n",
        "            price_variance = random.uniform(0.95, 1.05)\n",
        "            open_price = base_price * price_variance\n",
        "            \n",
        "            high_price = open_price * random.uniform(1.0, 1.02)\n",
        "            low_price = open_price * random.uniform(0.98, 1.0)\n",
        "            close_price = open_price * random.uniform(0.99, 1.01)\n",
        "            volume = random.uniform(100, 1000)\n",
        "            \n",
        "            data.append([\n",
        "                timestamp,                    # Open time\n",
        "                f\"{open_price:.2f}\",         # Open\n",
        "                f\"{high_price:.2f}\",         # High\n",
        "                f\"{low_price:.2f}\",          # Low\n",
        "                f\"{close_price:.2f}\",        # Close\n",
        "                f\"{volume:.2f}\",             # Volume\n",
        "                timestamp + 86400000,        # Close time\n",
        "                \"0\",                         # Quote asset volume\n",
        "                100,                         # Number of trades\n",
        "                \"0\",                         # Taker buy base asset volume\n",
        "                \"0\",                         # Taker buy quote asset volume\n",
        "                \"0\"                          # Ignore\n",
        "            ])\n",
        "        \n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "api-functions",
      "metadata": {},
      "outputs": [],
      "source": [
        "BINANCE_API = 'https://api.binance.com'\n",
        "\n",
        "# Initialize session based on mode\n",
        "if USE_MOCK_DATA:\n",
        "    SESSION = MockSession()\n",
        "else:\n",
        "    SESSION = requests.Session()\n",
        "    SESSION.headers.update({'User-Agent': 'crypto-alpha-lab/1.0'})\n",
        "\n",
        "def get_exchange_info() -> Dict:\n",
        "    \"\"\"Get exchange information from Binance API or mock data\"\"\"\n",
        "    try:\n",
        "        url = f'{BINANCE_API}/api/v3/exchangeInfo'\n",
        "        r = SESSION.get(url, timeout=20)\n",
        "        r.raise_for_status()\n",
        "        return r.json()\n",
        "    except Exception as e:\n",
        "        if not USE_MOCK_DATA:\n",
        "            print(f\"⚠️  Failed to fetch live data: {e}\")\n",
        "            print(\"💡 Consider setting USE_MOCK_DATA = True for offline testing\")\n",
        "        raise\n",
        "\n",
        "def top_spot_symbols(quote_priority: List[str] = None, limit: int = 25) -> List[str]:\n",
        "    \"\"\"Return top liquid spot symbols by quote asset priority and filters.\n",
        "    We approximate \"top\" by focusing on common quote assets and active trading status.\n",
        "    \"\"\"\n",
        "    if quote_priority is None:\n",
        "        quote_priority = ['USDT', 'USDC', 'FDUSD', 'BTC', 'ETH']\n",
        "    \n",
        "    info = get_exchange_info()\n",
        "    symbols = [s for s in info.get('symbols', []) if s.get('status') == 'TRADING' and s.get('isSpotTradingAllowed')]\n",
        "    \n",
        "    # Rank symbols by quote asset priority and base asset alphabetically as a tie-breaker\n",
        "    def score(sym):\n",
        "        q = sym.get('quoteAsset')\n",
        "        return (quote_priority.index(q) if q in quote_priority else 999, sym.get('baseAsset', ''))\n",
        "    \n",
        "    ranked = sorted(symbols, key=score)\n",
        "    picked = []\n",
        "    seen_bases = set()\n",
        "    \n",
        "    for s in ranked:\n",
        "        sym = s['symbol']\n",
        "        # Skip leveraged/index/fiat-like instruments by simple heuristics\n",
        "        if any(x in sym for x in ['UP', 'DOWN', 'BEAR', 'BULL']):\n",
        "            continue\n",
        "        if s.get('quoteAsset') not in quote_priority:\n",
        "            continue\n",
        "        # Prefer one quote per base to diversify the universe\n",
        "        base = s.get('baseAsset')\n",
        "        if base in seen_bases:\n",
        "            continue\n",
        "        seen_bases.add(base)\n",
        "        picked.append(sym)\n",
        "        if len(picked) >= limit:\n",
        "            break\n",
        "    \n",
        "    return picked\n",
        "\n",
        "def klines(symbol: str, interval: str = '1d', limit: int = 1000, start_time: int = None, end_time: int = None) -> pd.DataFrame:\n",
        "    \"\"\"Fetch OHLCV data for a symbol\"\"\"\n",
        "    try:\n",
        "        url = f'{BINANCE_API}/api/v3/klines'\n",
        "        params = {'symbol': symbol, 'interval': interval, 'limit': limit}\n",
        "        if start_time is not None: \n",
        "            params['startTime'] = start_time\n",
        "        if end_time is not None: \n",
        "            params['endTime'] = end_time\n",
        "        \n",
        "        r = SESSION.get(url, params=params, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        \n",
        "        cols = ['open_time','open','high','low','close','volume','close_time','quote_asset_volume','trades','taker_base_vol','taker_quote_vol','ignore']\n",
        "        df = pd.DataFrame(data, columns=cols)\n",
        "        \n",
        "        if df.empty:\n",
        "            return df\n",
        "            \n",
        "        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms', utc=True)\n",
        "        df['close_time'] = pd.to_datetime(df['close_time'], unit='ms', utc=True)\n",
        "        num_cols = ['open','high','low','close','volume']\n",
        "        df[num_cols] = df[num_cols].astype(float)\n",
        "        \n",
        "        return df[['open_time','open','high','low','close','volume','close_time']]\n",
        "        \n",
        "    except Exception as e:\n",
        "        if not USE_MOCK_DATA:\n",
        "            print(f\"⚠️  Failed to fetch data for {symbol}: {e}\")\n",
        "        raise\n",
        "\n",
        "def save_field_parquet(df: pd.DataFrame, symbol: str, field: str):\n",
        "    \"\"\"Save a single field to parquet file\"\"\"\n",
        "    assert field in ['open','high','low','close','volume'], f\"Invalid field: {field}\"\n",
        "    \n",
        "    # Each field to its own parquet per symbol, under storage/ohlcv/{field}/{symbol}.parquet\n",
        "    field_dir = os.path.join(STORAGE_DIR, field)\n",
        "    os.makedirs(field_dir, exist_ok=True)\n",
        "    path = os.path.join(field_dir, f'{symbol}.parquet')\n",
        "    \n",
        "    out = df[['open_time', field]].copy()\n",
        "    out = out.rename(columns={'open_time': 'timestamp', field: field})\n",
        "    out.to_parquet(path, index=False)\n",
        "    \n",
        "    print(f'Saved {field} -> {os.path.relpath(path)} | rows={len(out)}')\n",
        "\n",
        "def save_all_fields(df: pd.DataFrame, symbol: str):\n",
        "    \"\"\"Save all OHLCV fields for a symbol\"\"\"\n",
        "    for f in ['open','high','low','close','volume']:\n",
        "        save_field_parquet(df, symbol, f)\n",
        "\n",
        "def load_field(symbol: str, field: str) -> pd.DataFrame:\n",
        "    \"\"\"Load a specific field for a symbol from parquet\"\"\"\n",
        "    path = os.path.join(STORAGE_DIR, field, f'{symbol}.parquet')\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Data file not found: {path}\")\n",
        "    return pd.read_parquet(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fetch-data-section",
      "metadata": {},
      "source": [
        "## Fetch universe and OHLCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fetch-symbols",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get top symbols\n",
        "symbols = top_spot_symbols(limit=10)  # Reduced for testing\n",
        "print('Selected symbols:', symbols)\n",
        "print(f'Found {len(symbols)} symbols to process')\n",
        "\n",
        "# Download OHLCV data\n",
        "all_counts = {}\n",
        "errors = {}\n",
        "\n",
        "for sym in tqdm(symbols, desc='Downloading OHLCV (1d)'):\n",
        "    try:\n",
        "        df = klines(sym, interval='1d', limit=100)  # Reduced limit for testing\n",
        "        if df.empty:\n",
        "            print(f'⚠️  No data for {sym}')\n",
        "            continue\n",
        "            \n",
        "        save_all_fields(df, sym)\n",
        "        all_counts[sym] = len(df)\n",
        "        \n",
        "        # Be gentle with API calls\n",
        "        if not USE_MOCK_DATA:\n",
        "            time.sleep(0.1)\n",
        "            \n",
        "    except requests.HTTPError as e:\n",
        "        error_msg = f'HTTP error: {e}'\n",
        "        print(f'❌ {sym}: {error_msg}')\n",
        "        errors[sym] = error_msg\n",
        "    except Exception as e:\n",
        "        error_msg = f'Error: {e}'\n",
        "        print(f'❌ {sym}: {error_msg}')\n",
        "        errors[sym] = error_msg\n",
        "\n",
        "# Summary\n",
        "print(f'\\n📊 SUMMARY:')\n",
        "print(f'✅ Successfully processed: {len(all_counts)} symbols')\n",
        "print(f'❌ Errors encountered: {len(errors)} symbols')\n",
        "\n",
        "if all_counts:\n",
        "    print(f'\\n📈 Completed symbols: {list(all_counts.keys())}')\n",
        "    sample_counts = {k: all_counts[k] for k in list(all_counts)[:5]}\n",
        "    print(f'📋 Sample row counts:')\n",
        "    print(json.dumps(sample_counts, indent=2))\n",
        "else:\n",
        "    print('⚠️  No data was successfully downloaded')\n",
        "\n",
        "if errors:\n",
        "    print(f'\\n❌ Errors summary:')\n",
        "    for sym, error in errors.items():\n",
        "        print(f'  {sym}: {error}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "validation-section",
      "metadata": {},
      "source": [
        "## Validation: Reload parquet files and inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "validation-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate saved data by reloading and inspecting\n",
        "print('🔍 VALIDATION: Reloading and inspecting saved data\\n')\n",
        "\n",
        "# Get symbols that were successfully processed\n",
        "check_syms = list(all_counts.keys())[:3] if 'all_counts' in globals() and all_counts else []\n",
        "\n",
        "if not check_syms:\n",
        "    print('⚠️  No symbols to validate. Please run the data collection first.')\n",
        "else:\n",
        "    print(f'🧪 Validating data for: {check_syms}\\n')\n",
        "    \n",
        "    validation_success = 0\n",
        "    validation_errors = []\n",
        "    \n",
        "    for sym in check_syms:\n",
        "        print(f'📊 {sym}:')\n",
        "        \n",
        "        for field in ['open','high','low','close','volume']:\n",
        "            try:\n",
        "                dfv = load_field(sym, field)\n",
        "                sample_data = dfv.head(2).to_dict(orient='records')\n",
        "                print(f'  ✅ {field}: {dfv.shape} rows')\n",
        "                \n",
        "                # Quick sanity checks\n",
        "                if dfv.empty:\n",
        "                    raise ValueError(f'{field} data is empty')\n",
        "                \n",
        "                if field in ['open', 'high', 'low', 'close'] and dfv[field].min() <= 0:\n",
        "                    raise ValueError(f'{field} contains non-positive values')\n",
        "                \n",
        "                if field == 'volume' and dfv[field].min() < 0:\n",
        "                    raise ValueError(f'{field} contains negative values')\n",
        "                \n",
        "                validation_success += 1\n",
        "                \n",
        "                # Show sample data for first field only to avoid clutter\n",
        "                if field == 'open':\n",
        "                    print(f'     Sample: {sample_data}')\n",
        "                    \n",
        "            except Exception as e:\n",
        "                error_msg = f'{sym}.{field}: {e}'\n",
        "                print(f'  ❌ {field}: {e}')\n",
        "                validation_errors.append(error_msg)\n",
        "        \n",
        "        print()  # Empty line between symbols\n",
        "    \n",
        "    # Final validation summary\n",
        "    total_expected = len(check_syms) * 5  # 5 fields per symbol\n",
        "    print(f'\\n📋 VALIDATION SUMMARY:')\n",
        "    print(f'✅ Successful validations: {validation_success}/{total_expected}')\n",
        "    print(f'❌ Validation errors: {len(validation_errors)}')\n",
        "    \n",
        "    if validation_errors:\n",
        "        print('\\n❌ Error details:')\n",
        "        for error in validation_errors:\n",
        "            print(f'  - {error}')\n",
        "    \n",
        "    if validation_success == total_expected:\n",
        "        print('\\n🎉 ALL VALIDATIONS PASSED! Data collection and storage working correctly.')\n",
        "    else:\n",
        "        print(f'\\n⚠️  {total_expected - validation_success} validations failed.')\n",
        "\n",
        "# Additional check: verify storage structure\n",
        "print('\\n🗂️  STORAGE STRUCTURE CHECK:')\n",
        "try:\n",
        "    if os.path.exists(STORAGE_DIR):\n",
        "        field_dirs = [d for d in os.listdir(STORAGE_DIR) if os.path.isdir(os.path.join(STORAGE_DIR, d))]\n",
        "        print(f'📁 Field directories: {sorted(field_dirs)}')\n",
        "        \n",
        "        for field_dir in sorted(field_dirs):\n",
        "            field_path = os.path.join(STORAGE_DIR, field_dir)\n",
        "            parquet_files = [f for f in os.listdir(field_path) if f.endswith('.parquet')]\n",
        "            print(f'  📄 {field_dir}: {len(parquet_files)} files')\n",
        "    else:\n",
        "        print('❌ Storage directory does not exist')\n",
        "except Exception as e:\n",
        "    print(f'❌ Error checking storage structure: {e}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
